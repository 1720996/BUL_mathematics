\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{url,amsmath,graphicx,amssymb,booktabs}
\usepackage[top=1.5cm, bottom=1.5cm, left=2.5cm, right=2.5cm]{geometry}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}

\title{MA3614 - Complex Variable Methods and Applications}
\author{1720996}
\date{\today}

\begin{document}

\maketitle

\tableofcontents

\section{Introduction and Revision}
\subsection{Some definitions and notations}
\begin{definition}
    An ordinary differential equation (ODE) is an equation involving one or more derivatives of an unknown function of one variable.
\end{definition}
\begin{definition}
    The order of an ODE is the order of the highest derivative of the unknown function in the equation.
\end{definition}
\begin{definition}
    Suppose that $y$ is the unknown function and $x$ is the independent variable. Then the $ODE$ is linear (of order $m\in \mathbb{N}$) if one can represent it in the form:
    \begin{equation}
        a_m(x)\frac{d^m y}{dx^m} + a_{m-1}(x)\frac{d^{m-1}}{dx^{m-1}} + \ldots + a_1(x)\frac{dy}{dx}+a_)(x) y=f(x). \label{eq:order}
    \end{equation}
    Any other type of ODE is nonlinear.
\end{definition}
\begin{definition}
    A linear ODE is homogeneous if $f(x)\equiv 0$ in eq.~(\ref{eq:order}).
\end{definition}

\begin{table}[h]
\begin{tabular}{|c|c|c|c|c|}
\hline
   & ODE & Order & Linear & Homogeneous \\ \hline
 $1$ & $ 5\frac{d^2 y}{dx^2} - 6\frac{dy}{dx} + y = xe^x $ & $2$ & Yes & No \\ 
 $2$ & $xy\frac{dy}{dx} + \frac{d^4 y}{dx^4} = \sin{x}$ & $4$ & No & No \\
 $3$ & $x^2\frac{d^2 y}{dx^2} + x\frac{dy}{dx} + (x^2 - 1)y = 0$ & $2$ & Yes & Yes \\
 $4$ & $\left( y+\frac{d^3 y}{dx^3} \right)^2 + \frac{dy}{dx} = ye^x$ & $3$ & No & No \\
 $5$ & $x^2\frac{d^2 y}{dx^2}+y\cos{x} = x^2$ & $2$ & Yes & No \\ \hline
\end{tabular}
\end{table}
\subsection{First-order ODEs}
We can write a general form for a first-order ODE as
\begin{equation}
    F(x,y,y^\prime) = 0,\nonumber
\end{equation}
where $F$ is a given function of three variables.
\subsubsection{Direct Integration}
This is appropriate for ODEs of the type:
\begin{equation}
    \frac{dy}{dx} = f(x).
\end{equation}

\subsubsection{The method of seperation of variables}
This method is appropriate for the ODEs of the type:
\begin{equation}
    \frac{dy}{dx} = p(x)q(y),
\end{equation}
where $p(x)$ and $q(x)$ are known functions.

\subsubsection{Integrating factor method}
General first order linear ODEs of the form:
\begin{equation}
    \frac{dy}{dx} + P(x)y = Q(x)
\end{equation}
can be solved using an integrating factor $\lambda$, where
\begin{equation}
    \lambda(x) = e^{\int P(x)\,dx}.
\end{equation}
Hence,
\begin{equation}
    y(x) = \frac{1}{\lambda(x)}\int \lambda(x)Q(x)\,dx+\frac{C}{\lambda(x)}.
\end{equation}

\section{Second order linear ODEs}
\subsection{Some theory}
A second order linear ODE has the general form
\begin{equation}
    a_2(x)y^{\prime\prime} + a_1(x)y^\prime + a_0(x)y = f(x).
\end{equation}
\begin{theorem}
    Consider the IVP
    \begin{equation}
        y^{\prime\prime} + \tilde{a}_1(x)y^\prime + \tilde{a}_0(x)y = 0,\quad y(x_0)=\alpha,\quad y^\prime(x_0)=\beta
    \end{equation}
    where $\tilde{a}_1(x)$, $\tilde{a}_0$ are continuous on an open interval, say $(a,b)$, containing $x_0$. There is exactly one solution $y=\phi(x)$ to the IVP and the solution exists throughout $(a,b)$.
\end{theorem}

\subsection{Second order linear ODEs with constant coefficients}
This is in the form:
\begin{equation}
    a_2\frac{d^2 y}{dx^2} + a_1\frac{dy}{dx} + a_0 y = f(x)
\end{equation}
where $a_0,\,a_1,\,a_2$ are constants.

\subsubsection{The homogeneous case}

\begin{itemize}
    \item \textbf{Real distinct roots: }Let the roots be $m_1$ and $m_2$ where $m_1\neq m_2$. Then $y_1 = e^{m_1 x}$ and $y_2 = e^{m_2 x}$ are both solutions. The general solution is the linear combination:
    \begin{equation}
        y = Ae^{m1_x} + Be^{m_2 x}.
    \end{equation}
    \item \textbf{Real repeated roots: }Let the roots be equal so that $m_1=m_2$. Then one solution is $y_1 = e^{m_1 x}$ and the second is $y=xe^{m_1}$. Thus the general solution is:
    \begin{equation}
        y = (Ax + B)e^{m_1 x}.
    \end{equation}
    \item \textbf{Complex roots: }Let the roots be complex such that $m_1 = \alpha+i\beta$ and $m_2 = \alpha - i\beta$ where $\alpha$ and $\beta$ are real numbers. There are two distinct solutions to the ODE, namely $y_1 = e^{(\alpha + i\beta)x}$ and $y_2 = e^{(\alpha - i\beta)x}$. The general solutions is a linear combination which may be written as:
    \begin{equation}
        y = e^{\alpha x}(Ae^{i\beta x}+Be^{-i\beta x}).
    \end{equation}
    Equivalently:
    \begin{equation}
        y = e^{\alpha x}[C\cos{(\beta x)}+D\sin{(\beta x)}],
    \end{equation}
    where the two arbitrary constants are $C=A+B$ and $D = i(A+B)$.
\end{itemize}

\subsubsection{The particular integral}
Look through examples.

\subsection{Euler equations (homogeneous)}
This class of ODE has the general form:
\begin{equation}
    ax^2\frac{d^2 y}{dx^2} + bx\frac{dy}{dx} + cy = f(x)
\end{equation}
where $a,\, b$ are constants. 
\begin{itemize}
    \item \textbf{Real distinct roots: }Let the roots be $m_1$ and $m_2$ where $m_1\neq m_2$. Then $y_1 = x^{m_1}$ and $y_2 = x^{m_2}$ are both solutions. The general solution is the following:
    \begin{equation}
        y = Ae^{\frac{m_1}{2}\ln{x^2}} + Be^{\frac{m_2}{2}\ln{x^2}},\quad x<0\,\text{ or }\, x>0.
    \end{equation}
    \item \textbf{Real repeated roots: }Let the roots be equal so that $m_1=m_2$. The one solution to the ODE is $y_1 = x^{m_1}$ and the second is $y = x^{m_1}\ln{x}$. Thus, the general solution is:
    \begin{equation}
        y = (\tilde{A}\ln{x^2}+B)e^{\frac{m_1}{2}\ln{x^2}},\quad x<0\,\text{ or }\,x>0,
    \end{equation}
    where $\tilde{A} = A/2$ is arbitrary.
    \item \textbf{Complex roots: }Let the roots be complex such that $m_1 = \alpha+i\beta$ and $m_2 = \alpha - i\beta$ where $\alpha$ and $\beta$ are real numbers. There are two distinct solutions to the ODE, namely $y_1 = x^{(\alpha + i\beta)}$ and $y_2 = x^{(\alpha - i\beta)}$. The general solutions is a linear combination which may be written as:
    \begin{equation}
        y = e^{\frac{\alpha}{2}\ln{x^2}}\left[ C\cos{\left( \frac{\beta}{2}\ln{x^2} \right)} + D\sin{\left( \frac{\beta}{2}\ln{x^2} \right)} \right],\quad x<0\,\text{ or }\, x>0.
    \end{equation}
\end{itemize}
\textbf{Note: }For an Euler equation of the form
\begin{equation}
    a(x-x_0)^2\frac{d^2 y}{dx^2} + b(x-x_0)\frac{dy}{dx} + cy = 0,
\end{equation}
the solution is exactly the same except that we look for solutions of the form $y = (x-x_0)^m$.

\subsection{Linearly independent solutions}
\begin{theorem}
    If $y_1$ and $y_2$ are two solutions of the differential equation
    \begin{equation}
        y^{\prime\prime}+\tilde{a}_1(x)y^\prime + \tilde{a}_0(x)y = 0, \label{eq:LIS}
    \end{equation}
    then the linear combination
    \begin{equation}
        y = Ay_1 + By_2 \label{eq:lincom}
    \end{equation}
    is also a solution for any values of $A$ and $B$.
\end{theorem}
\begin{definition}
    Two solutions, $y_1$ and $y_2$, of eq.~(\ref{eq:LIS}) form a fundamental set of solutions if every solution of eq.~(\ref{eq:LIS}) can be expressed as a linear combination of the form eq.~(\ref{eq:lincom}). 
\end{definition}
\begin{definition}
    Two functions, $y_1(x)$ and $y_2(x)$ are said to be linearly independent on an interval $(a,b)$ if the only solution to $Ay_1(x)+By_1(x) = 0$ on $(a,b)$ is $A=B=0$. 
\end{definition}
\begin{theorem}
    Let $\tilde{a}_1(x)$ and $\tilde{a}_0(x)$ be continuous on $(a,b)$ and let $y_1$ and $y_2$ be two solutions of the differential eq.~(\ref{eq:LIS}). Then the following three statements are equivalent:
    \begin{enumerate}
        \item $y_1$ and $y_2$ are a fundamental set;
        \item $y_1$ and $y_2$ are linearly independent;
        \item the Wronskian $W(y_1,y_2;x_0)$ does not vanish at any interior point $x_0$ of $(a,b)$ (it could be, however, vanish as $x_0\to a$ or $x_0\to b$).
    \end{enumerate}
\end{theorem}
\begin{theorem}
    Let $\tilde{a}_1(x)$ and $\tilde{a}_0(x)$ be continuous on $(a,b)$, then the Wronskian of the differential eq.~(\ref{eq:LIS}) is given by
    \begin{equation}
        W=C\exp \left\{ -\int\tilde{a}_1(x)\, dx \right\}.
    \end{equation}
\end{theorem}

\subsection{Reduction of order}
Look through examples.

\subsection{Normal Form ODEs}
Look through examples.

\subsection{ODEs with singular points}
Consider the general seconds order linear ODE
\begin{equation}
    a_2(x)y^{\prime\prime}+a_1(x)y^\prime + a_0(x)y = f(x).
\end{equation}
The solution of such ODEs in an interval, say $(a,b)$, is closely linked to the behaviour of $a_2(x)$ in that interval.
\begin{itemize}
    \item A point $x_0\in(a,b)$ such that $a_2(x_0)\neq 0$ is called an ordinary point.
    \item A point $x_0\in(a,b)$ such that $a_2(x_0) = 0$ but $a_1(x_0)\neq 0$ and/or $a_0(x_0)\neq 0$ is called a singular point.
\end{itemize}
A singular point is a regular singular point if
\begin{equation}
    \lim_{x\to x_0}(x-x_0)\frac{a_1(x)}{a_2(x)}\quad \text{and}\quad \lim_{x\to x_0}(x-x_0)^2\frac{a_0(x)}{a_2(x)}
\end{equation}
both exist. A singular point that does not satisfy this is called an irregular singular point.

\section{Perturbation techniques}
\subsection{Regular Perturbation}
Use examples.

\subsection{The WKB method}
Use examples.

\section{Sturm-Liouville theory}
\subsection{Introduction to eigenvalues and eigenfunctions}
Use examples.

\subsection{Sturm-Liouville eigenvalue problems}
Use examples.

\begin{theorem}
    All the eigenvalues of the Sturm-Liouville problem are real.
\end{theorem}

\begin{theorem}
    If $\phi_1(x)$ and $\phi_2(x)$ are two eigenfunctions of the Sturm-Liouville problem with corresponding eigenfunctions $\lambda_1$ and $\lambda_2$, $\lambda_1\neq \lambda_2$, then
    \begin{equation}
        \int_0^1 r(x)\phi_1 \phi_2\,dx=0.
    \end{equation}
\end{theorem}
\begin{theorem}
    The eigenvalues of the Sturm-Liouville problem are all simple. That is, to each eigenvalue there corresponds only one linearly independent eigenfunction. Further, the eigenvalues form an infinite sequence and can be ordered according to increasing magnitude so that:
    \begin{equation}
        \lambda_1<\lambda_2<\lambda_3<\lambda_4<\ldots<\lambda_n<\ldots.
    \end{equation}
    Moreover, $\lambda_n\to\infty$ as $n\to\infty$.
\end{theorem}



\section{Week 3}


\subsection{Linearly independent solutions}
\subsubsection{The principle of superposition}
\begin{theorem}
    If $y_1$ and $y_2$ are two solutions of the differential equation
    \begin{equation}
        y^{\prime\prime}+\tilde{a}_1(x)y^\prime + \tilde{a}_0(x)y=0, \label{eq:linear_ODE}
    \end{equation}
    then the linear combination
    \begin{equation}
        y = Ay_1 + By_2
    \end{equation}
    is also a solution for any values of $A$ and $B$.
\end{theorem}
\begin{definition}
    Two solutions, $y_1$ and $y_2$, of ODE~(\ref{eq:linear_ODE}) form a fundamental set of solutions if every solution of equation~(\ref{eq:linear_ODE}) can be expressed as a linear combination of $y_1$ and $y_2$,
    \begin{equation}
        y = Ay_1 + By_2. \label{eq:lin_comb}
    \end{equation}
\end{definition}
\begin{definition}
    The Wronskain of $y_1,\,y_2$ at the point $x_0$ is expressed as
    \begin{equation}
        W(y_1,y_2;x_0) = \left\vert \begin{array}{cc}
             y_1 (x_0) & y_2 (x_0) \\
             y_1^\prime (x_0) & y_2^\prime (x_0)
        \end{array} \right\vert.
    \end{equation}
\end{definition}
Provided a Wronskian is not zero, then for all inital values $\gamma$ and $\delta$ there exists a unique linear combination~\ref{eq:lin_comb} of solutions $y_1$ and $y_2$ satisfying the initial conditions 
\begin{equation}
    y(x_0) = \gamma,\quad y^\prime(x_0) = \delta.
\end{equation}
\begin{definition}
    Two functions, $y_1(x)$ and $y_2(x)$ are said to be linearly independent on an interval $(a,b)$ if the only solution to $Ay_1(x) + By_2(x) = 0$ on $(a,b)$ is $A=B=0$. 
\end{definition}
\begin{theorem}
    Let the coefficients $\tilde{a}_1(x)$ and $\tilde{a}_0(x)$ be continuous on $(a,b)$ and let $y_1$ and $y_2$ be two solutions of the ODE
    \begin{equation}
        y^{\prime\prime}+\tilde{a}_1(x)y^\prime + \tilde{a}_0(x)y=0.
    \end{equation}
    Then the following three statements are equivalent:
    \begin{enumerate}
        \item $y_1$ and $y_2$ are a fundamental set;
        \item $y_1$ and $y_2$ are linearly independent;
        \item The Wronskian $ W(y_1,y_1;x_0) $ does not vanish at any interior point $x_0$ of $(a,b)$ (it could, however, vanish at $x_0\to a$ or $x_0\to b$).
    \end{enumerate}
\end{theorem}


\subsection{Linearly independent solutions: Wronskian in Integral Form}
\begin{theorem}
    Let $\tilde{a}_1(x)$ and $\tilde{a}_0(x)$ be continuous on $(a,b)$, then Wronskian of the ODE
    \begin{equation}
        y^{\prime\prime}+\tilde{a}_1(x)y^\prime + \tilde{a}_0(x)y=0
    \end{equation}
    is given by
    \begin{equation}
        W = Ce^{-\int \tilde{a}_1(x)\, dx}.
    \end{equation}
\end{theorem}


\subsection{Reduction of Order: Homogeneous case}
See examples.


\section{Week 4}
\subsection{Reduction of order: Inhomogeneous case}
See examples.


\subsection{Normal form ODEs}
\begin{definition}
    A second order linear ODE is in normal form if it does not contain $y^\prime$.
\end{definition}
$y^{\prime\prime}+6y=1$ is in normal form, but $y^{\prime\prime}+xy^\prime +6y=1$ is not.
Consider the ODE
\begin{equation}
    y^{\prime\prime}+\tilde{a}_1(x)y^\prime + \tilde{a}_0(x)y=f(x).\label{eq:inhomo_ODE}
\end{equation}
To reduce~\ref{eq:inhomo_ODE} to
\begin{equation}
    u^{\prime\prime} +I(x)u = s(x),
\end{equation}
represent $y(x) = t(x)u(x)$ and solve the coefficient of $u^\prime$ to be zero.


\subsection{ODEs with Singular Points}
The solution to general second order ODEs in an interval $(a,b)$ is closely linked to the behaviour of $a_2(x)$ in that interval.
\begin{itemize}
    \item A point $x_0\in(a,b)$ such that $a_2(x_0)\neq0$ is called an ordinary point.
    \item A point $x_0\in(a,b)$ such that $a_2(x_0)=-$ but $a_1(x_0)\neq0$ and/or $a_0(x_0)\neq0$ is called a singular point.
    \item $\tilde{a}_1(x) = a_1(x)/a_2(x)$ and $\tilde{a}_0(x) = a_0(x)/a_2(x)$ are continuous at an ordinary point.
    \item $\tilde{a}_1(x)$ or $\tilde{a}_0(x)$ or both will become unbounded (and hence discontinuous) at any singular point.
    \item A singular point is a regular singular point if
    \begin{equation}
        \lim_{x\to x_0}(x-x_0)\frac{a_1(x)}{a_2(x0}\quad\text{and}\quad\lim_{x\to x_0}(x-x_0)^2\frac{a_0(x)}{a_2(x)}
    \end{equation}
    both exist. A singular point not satisfying this is called an irregular singular point.
\end{itemize}


\section{Week 5}
\subsection{Regular Perturbation}
Use examples.

\section{Week 7}
\subsection{The WKD perturbation method}
Use examples.


\section{Week 9}
\subsection{Introduction to BVP eigenvalues and eigenvectors}
\begin{theorem}
    Consider the IVP
    \begin{align}
         y^{\prime\prime}+p(x)y^\prime + q(x)y=0,\quad a<x<b, \\
         y(x_0) = \alpha,\\
         y^\prime(x_0) = \beta.
    \end{align}
    where functions $p$ and $q$ are continuous on an open interval $(a,b)$, containing $x_0$. Then there exists a solution $y$ on the interval $(a,b)$ and the solution is unique.
\end{theorem}
\begin{corollary}
    All homogeneous BVPs have a trivial solution, $y=0.$
\end{corollary}
\begin{corollary}
    Generally, the eigenvalues $\lambda_n$ of a BVP are those values $\lambda$ for which a non-trivial solution to the BVP exists.\\
    The non-trivial solutions, corresponding to $\lambda_n$ are the eigenfunctions, $\{ \phi_n \}$, of the BVP.
\end{corollary}
\subsubsection{The General Case}
Consider the BVP:
\begin{align}
    y^{\prime\prime}+P(x,\lambda)y^\prime + Q(x,\lambda)y=0,\quad 0\leq x\leq 1, \\
    a_1y(0) + a_2y^\prime(0) = 0, \\
    b_1y(1) + b_2y^\prime(1) = 0;
\end{align}
$a_1,\,a_2,\,b_1$ and $b_2$ are real constants. $P$ and $Q$ are given functions of their arguments, $\lambda$ is a parameter.\\
Assume that the functions $P(x,\lambda)$ and $Q(x,\lambda)$ are continuous for $0\leq x\leq 1$ and for all values of $\lambda$.\\
The general solution to the ODE is:
\begin{equation}
    y = Ay_1(x,\lambda) + By_2(x,\lambda),\nonumber    
\end{equation}
where $y_1$ and $y_2$ are a fundamental pair (every solution of the ODE can be expressed as a linear combination of $y_1$ and $y_2$.\\
Boundary conditions give
\begin{align}
    \begin{pmatrix}
    a_1y_1(0,\lambda) + a_2y^\prime_1(0,\lambda) & a_1y_2(0,\lambda) + a_2y^\prime_2(0,\lambda) \\
    b_1y_1(1,\lambda) + b_2y^\prime_1(1,\lambda) & b_1y_2(0,\lambda) + b_2y^\prime_2(0,\lambda)
    \end{pmatrix}
    \begin{pmatrix}
    A \\ B
    \end{pmatrix}
    =
    \begin{pmatrix}
    0 \\ 0
    \end{pmatrix}.
\end{align}
Therefore,
\begin{equation}
    D(\lambda) = \left\lvert \begin{array}{cc}
         a_1y_1(0,\lambda) + a_2y^\prime_1(0,\lambda) & a_1y_2(0,\lambda) + a_2y^\prime_2(0,\lambda) \\
        b_1y_1(1,\lambda) + b_2y^\prime_1(1,\lambda) & b_1y_2(0,\lambda) + b_2y^\prime_2(0,\lambda)
    \end{array} \right\rvert.
\end{equation}
For non-trivial solution, $D(\lambda)$ = 0; the $\lambda$ that satisfy $D(\lambda)=0$ are the eigenvalues of the BVP.

\section{Week 10}
\subsection{Sturm-Liouville eigenvalue problem}
\subsubsection{The problem itself}
Consider the BVP:
\begin{align}
    [p(x)y^\prime]^\prime - q(x)y + \lambda r(x)y = 0,\quad 0\leq x\leq 1, \\
    a_1y(0)+a_2y^\prime(0) = 0,\quad b_1y(1)+b_2y^\prime(1) = 0.
\end{align}
Here, $p(x),\,p^\prime(x),\,q(x),\,r(x)$ are real and continuous and $p(x),\,r(x)>0$ on $0\leq x\leq 1$.

\subsubsection{Properties of the S-L BVP}
\begin{theorem}
    All the eigenvalues of the S-L problem are real.
\end{theorem}
\begin{theorem}
    If $\phi_1(x)$ and $\phi_2(x)$ are two eigenfunctions of the S-L problem corresponding to the eigenvalues $\lambda_1$ and $\lambda_2$, $\lambda_1\neq \lambda_2$, then
    \begin{equation}
        \int_0^1 r(x)\phi_1\phi_2\,dx=0.
    \end{equation}
\end{theorem}
\begin{theorem}
    The eigenvalues of the S-L problem are all simple. This means, to each eigenvalue there corresponds only one linearly independent eigenfunction. \\
    The eigenvalues can be ordered into an increasing infinite sequence:
    \begin{equation}
        \lambda_1<\lambda_2<\lambda_3<\lambda_4<\cdots<\lambda_n<\cdots 
    \end{equation}
    Moreover, $\lambda_n\to\infty$ as $n\to\infty$.
\end{theorem}

\subsubsection{Normalisation of the eigenfunctions}
Let $\phi_n(x)$ be an eigenfunction of the S-L problem. Then  so is $\varphi_n(x)=d_n\phi_n(x)$, for any constants $d_n$.\\
Chose
\begin{align}
    d_n &= \frac{1}{\sqrt{\int_0^1 r(\xi)\phi_n^2(\xi)\,d\xi}},\\
    \implies \varphi_n(x) = d_n\phi_n(x) &= \frac{\phi_n(x)}{\sqrt{\int_0^1 r(\xi)\phi_n^2(\xi)\,d\xi}},\\
    \implies \int_0^1 r(x)\phi_n^2(x)\,dx &= 1,
\end{align}
where $\varphi_n(x),\,n=1,\,2,\,3,\ldots$ are normalised eigenfunctions. They form an orthonormal set with respect to the weight function $r(x)$.
\begin{definition}
    Kronecker delta-symbol:
    \begin{equation}
        \delta_{mn}:= \left\{ \begin{array}{cc}
            1, & m=n \\
            0, & m\neq n
        \end{array} \right.
    \end{equation}
\end{definition}
\begin{definition}
    Orthogonality:
    \begin{equation}
        \int_0^1 r(x) \varphi_m(x) \varphi_n(x)\,dx = \delta_{mn}.
    \end{equation}
\end{definition}
\subsubsection{Generalised Fourier series}
\begin{theorem}
    Let $\varphi_n,\,n=1,\,2,\ldots$ be the normalised eigenfunctions of the S-L BVP. Let $f$ be an arbitrary piece-wise continuous function with piece-wise continuous derivative $f^\prime$ on the closed interval $[0,1]$. Then $f$ can be expressed as a generalised Fourier series:
    \begin{align}
        f(x) &= C_1\varphi_1(x) + C_2\varphi_2(x) + C_3\varphi_3(x) +\ldots + C_n\varphi_n(x) + \ldots \nonumber \\
        &= \sum_{n=1}^\infty C_n\varphi_n(x),
    \end{align}
    where
    \begin{equation}
        C_n = \int_0^1 f(x)r(x)\varphi(x)\,dx,\quad n=1,\,2,\,3,\ldots
    \end{equation}
\end{theorem}
This converges to $[f(x^+) + f(x^-)]/2$ at each point $x$ of the open interval $(0,1)$.\\
particularly series converges to $f(x)$ at any point $x\in (0,1)$, where the function $f$ is continuous.

\begin{corollary}
    The coefficients $C_m$ can be found by multiplying the series by $r(x)\varphi_m(x)$ and then integrating, that is,
    \begin{align}
        \int_0^1 f(x)r(x)\varphi_m(x)\,dx &= \sum_{n=1}^\infty C_n\int_0^1 r(x)\varphi_m(x)\varphi_n(x)\,dx,\nonumber,\\
        &= \sum_{n=1}^\infty C_n\delta_{mn} = C_m,\quad m=1,\,2,\,3,\ldots
    \end{align}
\end{corollary}



\section{Week 17}

\section{Week 18}

\section{Week 19}

\section{Week 20}

\end{document}
