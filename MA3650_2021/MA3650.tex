\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{url,amsmath,graphicx,amssymb,booktabs}
\usepackage[top=1.5cm, bottom=1.5cm, left=2.5cm, right=2.5cm]{geometry}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}

\title{MA3650 - Numerical Methods for Differential Equations}
\author{Luke Dando}
\date{\today}

\begin{document}

\maketitle

\tableofcontents

\section{Revision of Taylor series and Newton's method}
\subsection{Lecture 1}
\subsubsection{Newton's method}
Let $x_0$ be an initial guess of a root to a function $f(x)$. Then
\begin{equation}
    x_{n+1} = x_n - \frac{f(x)}{f^\prime(x)},\quad n\geq0. \label{eq:newton}
\end{equation}

\subsection{Lecture 2}
\subsubsection{IVT: Intermediate Value Theorem}
\begin{theorem}
Let $f:[a,b]\to \mathbb{R}$ be a continuous function, then $f$ achieves all values of the interval $[f(a),f(b)]$.\\
As a corollary, if $f(a)\cdot f(b)<0$, then $f$ has a root on $(a,b)$ (in the interior of $[a,b]$).
\end{theorem}
\subsubsection{Complex convergence}
\begin{lemma}
    The map $g:(0,\infty)\to\mathbb{R},\,g(x) = ln(e/x)$, maps $(e,\infty)$ into $(-\infty, 0)$ and $(0, e)$
to $(0, e)$.
\end{lemma}
\begin{corollary}
    Newton's method for $f(x) = ln(x)$ is possible for for any $x_0\in(0,e)$, and impossible when $x_0>e$.
\end{corollary}

\section{Convergence of Newton's method}
\subsection{Lecture 3}
\subsubsection{Babylonian iteration}
Let $\alpha>0$. To get an approximate value for $\sqrt{\alpha}$, start with a crude guess $x_1$ and improve on the approximation using
\begin{equation}
    x_{n+1} = \frac{(x_n+\alpha/x_n)}{2},\quad n\geq1. \label{eq:bab}
\end{equation}
\subsubsection{Error estimates}
\begin{lemma}
    Let $x_n$ be given by eq.~(\ref{eq:bab}). Then,
    \begin{equation}
        x_{n+1}-\sqrt{\alpha}=(x_n-\sqrt{\alpha})^2/2x_n. \label{eq:error1}
    \end{equation}
\end{lemma}
\subsubsection{Convergence close to the square root}
When $x_n\approx \sqrt{\alpha}$,
\begin{equation}
    (x_{n+1}-\sqrt{\alpha}) = \frac{(x_{n}-\sqrt{\alpha})^2}{2x_n} \approx \frac{(x_{n}-\sqrt{\alpha})^2}{2\sqrt{\alpha}}
\end{equation}
We define the error $e_n$ (at iterate $n$ as
\begin{equation}
    e_n = x_n - \sqrt{\alpha},
\end{equation}
allowing us to rewrite the estimate eq.~(\ref{eq:error1}) as
\begin{equation}
    e_{n+1} \approx \frac{e_n^2}{2\sqrt{\alpha}}
\end{equation}

\subsection{Lecture 4}
\subsubsection{Newton's method theory}
\begin{theorem}
    Let $I=[a,b]$ and $f:I\to\mathbb{R}$ be twice continuously differentiable. Suppose that
    \begin{equation}
        f(a)\cdot f(b)<0 \nonumber
    \end{equation}
    and that there are constants m and M, such that
    \begin{equation}
        0<m\leq \vert f^\prime(x)\vert\quad and \quad \vert f^{\prime\prime}(x)\vert \leq M
    \end{equation}
    for all $x\in I$. Let $K=M/2m$. Then, choose a root $r$ of $f$ in $I$ and $0<\delta<1/K$ such that $J=[r-\delta,r+\delta]\subseteq I$. Then, for any $x_0\in J$, the sequence defined by Newton's method in eq.~(\ref{eq:newton}) belongs to $J$ and ${x_n}_{n=1}^\infty$ converges to $r$. Moreover
    \begin{equation}
        \vert x_{n+1}-r\vert \leq K\vert x_n-r\vert^2,\quad n\geq0,
    \end{equation}
    so the convergence is quadratic.
\end{theorem}


\section{Wrapping up chapter 1}
\subsection{Lecture 6}
\subsubsection{Absolute and relative error}
\begin{definition}
    If $a$ is a number and $\hat{a}$ is an approximation to $a$, the absolute error is $\vert a-\hat{a}\vert$ and the relative error is $\frac{\vert a-\hat{a}\vert}{\vert a \vert}$ provided $a\neq0$.  
\end{definition}


\section{Finite differences}
\subsection{Lecture 7}
\subsubsection{Euler scheme}
For every $0\leq i \leq n$, we denote by $v_i$ the approximation of $u(t_i)$, the value of the exact solution at $t=t_i$. We approximate by the forward difference
\begin{equation}
    u^\prime (t_i) \approx \frac{v_{i+1} - v_i}{h}
\end{equation}
\subsubsection{Two dimensional grids}
Consider a function $u:\Omega \to \mathbb{R} $ where $\Omega = I\times J$ is the cartesian product of two real intervals, say $x\in I$ and $t\in J$. Although $I$ and $J$ can be infinite, numerical schemes consider finite grids, although they can be of arbitrary length. Let $I=[a,b]$ and $J=[c,d]$. We approximate $u$ at grid points $(x_i,t_j)$ such that
\begin{enumerate}
    \item $x_i=a+i(\delta x),\,0\leq i\leq N$ with $N\cdot(\delta x)=b-a$, that is, $\delta x=(b-a)/N$. The quantity $\delta x$ is the spatial step-size. 
    \item $t_j=c+j(\delta t),\,0\leq j\leq M$ with $m\cdot(\delta t)=d-c$, that is, $\delta t=(d-c)/M$. The quantity $\delta t$ is the temporal step-size. 
\end{enumerate}
\subsubsection{Standard approximations}
We denote by $v_{i,j}$ the numerical scheme approximation of $u(x_i,t_j)$. We shall need the following approximations.
\begin{enumerate}
    \item \textbf{Forward difference}:
    \begin{equation}
        \left(\frac{\partial u}{\partial x}\right)_{i,j} \approx \frac{v_{i+1,j}-v_{i,j}}{\delta x},
    \end{equation}
    \item \textbf{Backward difference}:
    \begin{equation}
        \left(\frac{\partial u}{\partial x}\right)_{i,j} \approx \frac{v_{i,j}-v_{i-1,j}}{\delta x},
    \end{equation}
    \item \textbf{Central (or symmetric) differences}:
    \begin{equation}
        \left(\frac{\partial u}{\partial x}\right)_{i,j} \approx \frac{v_{i+1,j}-v_{i-1,j}}{2(\delta x)},
    \end{equation}
    \item \textbf{Second order difference}:
    \begin{equation}
        \left(\frac{\partial^2 u}{\partial x^2}\right)_{i,j} \approx \frac{v_{i+1,j}+v_{i-1,j}-2v_{i,j}}{(\delta x)^2}.
    \end{equation}
    This is obtained by using the forward difference of $\frac{\partial^2 u}{\partial x^2}$, where we use the backward difference for the first derivative at $(x_{i+1},t_j)$ and the forward difference for the derivative at $(x_i,t_j)$.
\end{enumerate}


\subsection{Lecture 8}
\subsubsection{Errors for the standard approximation}
The error for the forward difference approximation is 
\begin{equation}
    \left\vert f^\prime (a)-\frac{f(a+h)-f(a)}{h} \right\vert = \vert f^{\prime\prime}(\xi)\vert \frac{h}{2}\leq \max_{x\in [a,a+h]}(\vert f^{\prime\prime}(x)\vert)\frac{h}{2}.
\end{equation}
The error is linear in $h$.\\
For the central difference approximation, the error is
\begin{equation}
    \left\vert f^\prime (a)-\frac{f(a+h)-f(a-h)}{2h} \right\vert = \vert f^{\prime\prime\prime}(\xi)-f^{\prime\prime\prime}(\zeta)\vert \frac{h^2}{12}\leq \max_{x\in [a-h,a+h]}(\vert f^{\prime\prime\prime}(x)\vert)\frac{h^2}{6}.
\end{equation}
The error is now quadratic in $h$.

\subsubsection{Order symbol}
\begin{definition}
    Let $f:I_0\to\mathbb{R}$ be a real valued function, we say that $f$ is of order not exceeding, or bounded with respect to, $x^\alpha$ as $x$ tends to $0$, denoted by
    \begin{equation}
        f=O(x^\alpha), \nonumber
    \end{equation}
    if there exists $\alpha\in \mathbb{R},\,C>0$ such that 
    \begin{equation}
        \vert f(x) \vert \leq C\vert x \vert^\alpha
    \end{equation}
    for all $x\in I_0$.
\end{definition}
\subsubsection{Local error of standard finite differences}
Consider a grid $(x_i,t_j)$ of step-sizes $\delta x$ and $\delta t$, respectively. Let $u_j$ represent $u(x_i,t_j)$. The following relations are exact:
\begin{enumerate}
    \item $\frac{\partial u}{\partial x}(x_i,t_j) = \frac{u_{i+1,j}-u_{i,j}}{\delta x} + O(\delta x)$,
    \item $\frac{\partial u}{\partial x}(x_i,t_j) = \frac{u_{i,j}-u_{i-1,j}}{\delta x} + O(\delta x)$,
    \item $\frac{\partial u}{\partial x}(x_i,t_j) = \frac{u_{i+1,j}-u_{i-1,j}}{2(\delta x)} + O((\delta x)^2)$,
    \item $\frac{\partial u}{\partial x}(x_i,t_j) = \frac{u_{i+1,j}+u_{i-1,j}-2u_{i,j}}{(\delta x)^2} + O(\delta x)$.
\end{enumerate}

\section{Explicit numerical schemes for the heat equation}
\subsection{Lecture 9}
\subsubsection{Explicit scheme of the heat equation}
\begin{equation}
    \frac{v_{i,j+1}-v_{i,j}}{(\delta t)}=\frac{v_{i+1,j}+v_{i-1,j}-2v_{i,j}}{(\delta x)^2}\nonumber,
\end{equation}
can be rearranged to give
\begin{equation}
    v_{i,j+1} = \rho v_{i-1,j}+(1-2\rho)v_{i,j}+\rho v_{i+1,j},\quad 1\leq i\leq N-1,\, 1\leq j \leq M-1, \label{eq:1dheatexpl}
\end{equation}
where the values of $u$ at $t_{j+1}$ are given from the values at $t_j$ and $\rho$ is the Courant number $\frac{\delta t}{(\delta x)^2}$.
\subsection{Lecture 10}
\subsubsection{Matrix form for the explicit heat equation}
We can rewrite eq.~(\ref{eq:1dheatexpl}) as 
\begin{equation}
    \underline{v}_{j+1}=A\underline{v}_j,\quad 0\leq j\leq M-1,\nonumber
\end{equation}
where $A$ is the tridiagonal matrix
\begin{equation}
    A = 
    \begin{pmatrix}
        1-2\rho & \rho & 0 & \cdots & 0 & 0 \\
        \rho & 1-2\rho & \rho & \cdots & 0 & 0 \\
        0 & \rho & 1-2\rho & \cdots & 0 & 0 \\
        \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
        0 & 0 & 0 & \rho & 1-2\rho & \rho \\
        0 & 0 & 0 & \cdots & \rho & 1-2\rho  
    \end{pmatrix}.
\end{equation}

\section{Further numerical schemes for the heat equation}
\subsection{Lecture 11}
\subsubsection{Implicit scheme of the heat equation}
\begin{equation}
    \frac{v_{i,j}-v_{i,j-1}}{(\delta t)}=\frac{v_{i+1,j}+v_{i-1,j}-2v_{i,j}}{(\delta x)^2}\nonumber,
\end{equation}
can be rearranged to give
\begin{equation}
    -\rho v_{i-1,j}+(1+2\rho)v_{i,j}-\rho v_{i+1,j} = v_{i,j-1},\quad 1\leq i\leq N-1,\, 1\leq j \leq M-1, \label{eq:1dheatimpl}
\end{equation}
where the values of $u$ at $t_{j}$ are given from the values at $t_{-1j}$ and $\rho$ is the Courant number $\frac{\delta t}{(\delta x)^2}$.
\subsubsection{Matrix form for the implicit heat equation}
The scheme in eq.~(\ref{eq:1dheatimpl}) can be written as
\begin{equation}
    B\underline{v}_j=\underline{v}_{j-1},\quad 1\leq j\leq M,\nonumber
\end{equation}
where $B$ is the tridiagonal matrix
\begin{equation}
    B = 
    \begin{pmatrix}
        1+2\rho & -\rho & 0 & \cdots & 0 & 0 \\
        -\rho & 1+2\rho & -\rho & \cdots & 0 & 0 \\
        0 & -\rho & 1+2\rho & \cdots & 0 & 0 \\
        \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
        0 & 0 & 0 & -\rho & 1+2\rho & -\rho \\
        0 & 0 & 0 & \cdots & -\rho & 1+2\rho  
    \end{pmatrix}.
\end{equation}
\subsubsection{Crank-Nicholson Scheme}
\begin{equation}
    \frac{v_{i,j+1}-v_{i,j}}{(\delta t)}=\frac{1}{(\delta x)^2}[\theta(v_{i-1,j+1}+2v_{i,j+1}+v_{i+1,j})+(1-\theta)(v_{i-1,j}-2v_{i,j}+v_{i+1,j})],
\end{equation}
where $\theta$ is a weight. The Crank-Nicholson scheme is written in matrix form by
\begin{equation}
    C\underline{v}_{j+1} = D\underline{v}_j \nonumber
\end{equation}
where matrices $C$ and $D$ are determined as before.


\section{Numerical schemes for complex boundary conditions}
\subsection{Lecture 13}
\subsubsection{}


\section{Numerical schemes for complex boundary conditions 2}
\subsection{Lecture 14}
\subsection{Lecture 15}
\subsection{Lecture 16}


\section{Matrix norms}
\subsection{Lecture 17}
\subsubsection{Vector Norms}
\begin{definition}
    A norm on $\mathbb{R}^n$ is a mapping $\left\Vert \ \right\Vert:\mathbb{R}^n\to\mathbb{R}$ satisfying the following axioms.
    \begin{enumerate}
        \item $\left\Vert \underline{x} \right\Vert\geq0$ for all vectors $\underline{x}\in \mathbb{R}^n$ with $\left\Vert \underline{x} \right\Vert=0$ iff $\underline{x}=\underline{0}$ (separation axiom).
        \item $\left\Vert \alpha\underline{x} \right\Vert=\vert\alpha\vert\cdot\left\Vert \underline{x} \right\Vert,\, \forall\alpha\in\mathbb{R},\,\forall\underline{x}\in\mathbb{R}^n$, (scaling axiom).
        \item $\left\Vert \underline{x}+\underline{y} \right\Vert\leq\left\Vert \underline{x} \right\Vert+\left\Vert \underline{y} \right\Vert,\,\forall\underline{x},\underline{y}\in\mathbb{R}^n$, (triangle inequality).
    \end{enumerate}
\end{definition}
\subsubsection{Matrix norms}
\begin{definition}
    A function $\left\Vert \ \right\Vert:M_n(\mathbb{R})\to\mathbb{R}$ is a matrix norm provided the following conditions are satisfied:
    \begin{enumerate}
        \item $\left\Vert A \right\Vert\geq0,\,\forall A\in M_n(\mathbb{R}),$ with $\left\Vert A \right\Vert=0$ iff $A=0$ (separation axiom).
        \item $\left\Vert A+B \right\Vert\leq\left\Vert A \right\Vert+\left\Vert B \right\Vert,\,\forall A,B\in M_n(\mathbb{R})$, the triangle inequality.
        \item $\left\Vert \alpha A \right\Vert = \vert\alpha\vert \cdot \left\Vert A \right\Vert,\,\forall A\in M_n(\mathbb{R}),\,\forall\alpha\in\mathbb{R}$, (scaling axiom).
        \item $\left\Vert AB \right\Vert\leq\left\Vert A \right\Vert\cdot\left\Vert B \right\Vert,\,\forall A,B\in M_n(\mathbb{R})$, the sub-multiplicative condition.
    \end{enumerate}
\end{definition}
\subsubsection{Compatible norm conditions}
\begin{definition}
    We say that a given matrix $\left\Vert \ \right\Vert_M$ is compatible with a vector norm $\left\Vert \ \right\Vert_{\underline{v}}$ on $\mathbb{R}^n$ if
    \begin{equation}
        \left\Vert A\underline{x} \right\Vert_{\underline{v}} \leq \left\Vert A \right\Vert_{M}\left\Vert \underline{x} \right\Vert_{\underline{v}},\quad \forall A\in M_n(\mathbb{R}),\,\forall\underline{x}\in\mathbb{R}^n.\nonumber
    \end{equation}
\end{definition}
\begin{proposition}
    For any compatible matrix norm $\left\Vert \ \right\Vert$, $\rho(A)\leq\left\Vert A \right\Vert$.
\end{proposition}

\subsubsection{Subordinate norm condition}
\begin{definition}
    Given any vector norm $\left\Vert \ \right\Vert_{\underline{v}}$, we can define a corresponding matrix norm $\left\Vert \ \right\Vert_M$ which is said to be subordinate to $\left\Vert \ \right\Vert_{\underline{v}}$ by
    \begin{equation}
        \left\Vert A \right\Vert_M = \max_{\left\Vert x \right\Vert_{\underline{v}}=1}\left\Vert A\underline{x} \right\Vert_{\underline{v}}.
    \end{equation}
\end{definition}
\begin{proposition}
    The spectral norm ($2$-norm) of a matrix is subordinate to the Euclidean norm on $\mathbb{R}^n$.
\end{proposition}
\begin{proposition}
    \begin{enumerate}
        \item In general, $\left\Vert I_n \right\Vert\geq 1$.
        \item If a matrix norm $\left\Vert \ \right\Vert_M$ is subordinate to a vector norm, $\left\Vert I_n \right\Vert_M=1$.
        \item The Frobenius norm is not subordinate to any vector norm, but it is compatible with $\left\Vert \ \right\Vert_2$.
    \end{enumerate}
\end{proposition}

\section{Matrix perturbations and condition numbers}
\subsection{Lecture 18}
\subsubsection{Small norm perturbation of the identity}
\begin{theorem}
    Let $\left\Vert \ \right\Vert$ be a matrix norm subordinate to a vector norm. Let $E\in M_n(\mathbb{R})$ with $\left\Vert E \right\Vert<1$. Then $I_n-E$ is invertible and
    \begin{equation}
        \frac{1}{1+\left\Vert E \right\Vert}\leq\left\Vert (I_n-E)^{-1} \right\Vert\leq \frac{1}{1-\left\Vert E \right\Vert},
    \end{equation}
    as well as
    \begin{equation}
        \frac{\left\Vert E \right\Vert}{1+\left\Vert E \right\Vert}\leq\left\Vert (I_n-E)^{-1}-I_n \right\Vert\leq \frac{\left\Vert E \right\Vert}{1-\left\Vert E \right\Vert}.
    \end{equation}
\end{theorem}
\subsubsection{Perturbations of Linear Systems}
\begin{theorem}
    Given a subordinate matrix norm $\left\Vert \ \right\Vert$, a non-singular matrix $A$, two vectors $\underline{b},\delta\underline{b}\neq\underline{0}$ and a matrix $\delta A$ such that $\left\Vert \delta A \right\Vert<1/\left\Vert A^{-1} \right\Vert$, then $\underline{x}$ such that $A\underline{x}=\underline{b}$ and $(\underline{x}+\delta\underline{x})$ such that $(A+\delta A)(\underline{x}+\delta\underline{x}) = \underline{b} + \delta\underline{b}$ satisfy
    \begin{equation}
        \frac{\left\Vert \delta\underline{x} \right\Vert}{\left\Vert \underline{x} \right\Vert}\quad\leq\quad \frac{\left\Vert A \right\Vert\cdot \left\Vert A^{-1} \right\Vert}{1-\left\Vert \delta A \right\Vert\cdot \left\Vert A^{-1} \right\Vert}\left( \frac{\left\Vert \delta A \right\Vert}{\left\Vert A \right\Vert} + \frac{\left\Vert \delta\underline{b} \right\Vert}{\left\Vert \underline{b} \right\Vert} \right).
    \end{equation}
\end{theorem}

\subsection{Lecture 19}
\subsubsection{Condition numbers}
\begin{definition}
    Given a matrix norm $\left\Vert \ \right\Vert$ and a non-singular matrix $A$, the condition number of $A$ relative to the norm $\left\Vert \ \right\Vert$ is defined by
    \begin{equation}
        \kappa(A)=\left\Vert A \right\Vert\cdot\left\Vert A^{-1} \right\Vert.\nonumber
    \end{equation}
\end{definition}
\begin{proposition}
    \begin{enumerate}
        \item When $\left\Vert \ \right\Vert$ is a subordinate to some other vector norm, $\kappa(A)\geq1$. In particular, $\kappa(I_n)=1$.
        \item When $O$ is orthogonal ($O^{-1}=O^T$), $\kappa_2(O)=1$, so there are non-identity matrices with condition number $\kappa_2$ equal to $1$.
    \end{enumerate}
\end{proposition}
\begin{definition}
    A matrix $A$ is perfectly conditioned if $\kappa(A)=1$, it is well-conditioned if $\kappa(a)$ is `close to $1$' and is ill-conditioned if $\kappa(A)$ is `much larger than $1$'.
\end{definition}
\begin{proposition}
    Let A be an invertible matrix and $\left\Vert \ \right\Vert$ a matrix norm.
    \begin{enumerate}
        \item $\kappa(\alpha A)=\kappa(A)$ for any $\alpha\neq 0$.
        \item $\kappa(A^{-1})=\kappa(A)$.
        \item Let $\left\Vert \ \right\Vert$ be compatible with a vector norm a vector norm, let $\vert\lambda_{\max}\vert=\rho(A)$ be the largest eigenvalue of $A$ in modulus and $\vert \lambda_{\min}\vert>0$ be the smallest eigenvalue of $A$ in modulus. Then,
        \begin{equation}
            \kappa(A)\geq\frac{\vert\lambda_{\max}\vert}{\vert\lambda_{\min}\vert}. \nonumber
        \end{equation}
        \item If $\left\Vert \ \right\Vert$ is subordinate ($\left\Vert I_n \right\Vert$=1) and $\left\Vert A-I_n \right\Vert<1$, then
        \begin{equation}
            \kappa(A)\leq \frac{\left\Vert A \right\Vert}{1-\left\Vert A-I_n \right\Vert}.
        \end{equation}
    \end{enumerate}
\end{proposition}
\subsubsection{Spectral condition number and singular values of matrices}
\begin{proposition}
    Let $A$ be a normal matrix ($AA^T = A^TA$), in particular symmetric, then
    \begin{equation}
        \kappa_2(A) = \frac{\max\{\vert\lambda\vert:\lambda\in\sigma(A)\}}{\min\{\vert\lambda\vert:\lambda\in\sigma(A)\}}.\nonumber
    \end{equation}
\end{proposition}
\begin{proposition}
    Let A be an invertible matrix.
    \begin{enumerate}
        \item There exist two orthonormal matrices $O_1$ and $O_2$ such that
        \begin{equation}
            A=O_1 DO_2^T
        \end{equation}
        where $D$ is a diagonal matrix with positive entries.
        \item Moreover, let $\sigma_{\max},\,\sigma_{\min}$ be the largest, respectively smallest, singular value of $A$:
        \begin{equation}
            \kappa_2(A)=\frac{\sigma_{\max}}{\sigma_{\min}}.\nonumber
        \end{equation}
    \end{enumerate}
\end{proposition}
\section{Interpolation, Lagrange polynomials}
\subsection{Lecture 21}
\subsubsection{Construction of Lagrange Polynomials}
For the Lagrange polynomials, $y_i$ and $f(x_i)$ can be used interchangeably.
\begin{lemma}
    Given $n+1$ points $(x_i,y_i) \in \mathbb{R}^2,\,0\leq i\leq n$, with $x_i\neq x_j$ when $i\neq j$. The polynomials
\begin{equation}
    L_i(x) = \prod_{j=0,\,\neq i}^n \left(\frac{x-x_j}{x_i-x_j}\right),\quad 0\leq i \leq n,
\end{equation}
satisfy $L_i(x_i)=1$ and $L_i(x_j)=0$ for $j\neq i$.
\end{lemma}
\begin{definition}
    Given $n+1$ points $(x_i,y_i) \in \mathbb{R}^2,\,0\leq i\leq n$, with $x_i\neq x_j$ when $i\neq j$. The Lagrange polynomial $p$ is a polynomial of degree up to $n$ equal to
\begin{equation}
    p(x) = \sum_{i=0}^n y_iL_i(x).
\end{equation}
This is \textit{linear} if $n=1$ and \textit{quadratic} if $n=2$.
\end{definition}
\subsubsection{Vandermonde Method}
Given $p(x)=ax^2+bx+c$, with $a,\,b$ and $c$ to be determined from the conditions $p(x_i)=y_i,\, i=0,\,1,\,2$, we can input this into a matrix know as the Vandermonde matrix and solve it for values $a,\,b$ and $c$.
\begin{equation}
    \begin{pmatrix}  x_0^2 & x_0 & 1 \\ x_1^2 & x_1 & 1 \\ x_2^2 & x_2 & 1 \end{pmatrix}\begin{pmatrix} a \\ b \\ c \end{pmatrix} = \begin{pmatrix} y_0 \\ y_1 \\ y_2 \end{pmatrix}.
\end{equation}
This can be extended for larger polynomials trivially.

\subsection{Lecture 22}
\subsubsection{Error estimates for Lagrange interpolation}
\begin{proposition}
    Suppose $f\in C^{n+1}[a,b]$. Then, the error function $e$ between $f$ and it's Lagrange interpolant $p$ at $\{x_i\}_{i=0}^n$ is given by
    \begin{equation}
        e(x)=f(x)-p(x)=\frac{f^{(n+1)}(\xi)}{(n+1)!}(x-x_0)\cdots(x-x_n),\nonumber
    \end{equation}
    where $\xi=\xi(x)$ lies in $(a,b)$.
\end{proposition}


\section{Interpolation, splines}
\subsection{Lecture 23}
\subsubsection{Linear Splines}
The rule of $S_n$ on the interval $[x_i,\,x_{i+1}]$ is
\begin{equation}
    S_n(x) = y_i + \frac{x-x_i}{x_{i+1}-x_i}(y_{i+1}-y_i) = \left(\frac{x-x_i}{x_{i+1}-x_i}\right)y_{i+1} + \left(\frac{x_{i+1}-x}{x_{i+1}-x_i}\right)y_i. \nonumber
\end{equation}
We can also re-cast $S_n$ as
\begin{equation}
    S_n(x) = \left(\frac{x_{i+1}y_i - x_ky_{i+1}}{x_{i+1}-x_i}\right) + x\left(\frac{y_{i+1}-y_i}{x_{i+1}-x_i}\right),\quad x\in [x_i,\,x_{i+1}]
\end{equation}
to identify the gradient easily.

\subsection{Lecture 24}
Assume we have splines on $n$ intervals. The global regularity of the spline and the degree of the local polynomials satisfy the following relations. The notation $f\in C^k[a,\,b]$ means that $f$ is $k$-times continuously differentiable. Recall that this means that the function and its first $k$ derivatives are continuous at any $z\in[]a,\,b]$, that is,
\begin{equation}
    \lim_{x\to z^-}f^{(j)}(x) = \lim_{x\to z^+}f^{(j)}(x),\quad 0\leq j \leq k. \nonumber
\end{equation}
\begin{table}[h!]
\centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        polynomial & coefficients & $C^0$ & $C^1$ & $C^2$ & equations & free coefficients \\
        \hline
        linear & $2n$ & $2(n-1)+2$ &  &  & $2n$ & 0 \\
        \hline
        quadratic & $3n$ & $2(n-1)+2$ & $n-1$ &  & $3n-1$ & 1 \\
        \hline
        cubic & $4n$ & $2(n-1)+2$ & $n-1$ & $n-1$ & $4n-2$ & 2 \\
        \hline
    \end{tabular}
    \caption{Number of variables (coefficients) and equations for splines.}
    \label{table:splines}
\end{table}

\subsubsection{Natural Cubic Splines}
Given a function $f$ defined on $[a,b]$ and a set of points $a=x_0<x_1<\ldots<x_n=b$, a function $S$ is called a natural cubic spline if there exists $n$ cubic polynomials $S_i$ such that:
\begin{enumerate}
    \item $ S(x) = S_i(x) $ for $x$ in $[x_i,\,x_{i+1}]$ and $0\leq i\leq n-1$;
    \item $S_i(x) = f(x_i)$ and $S_i(x_{i+1}) = f(x_{i+1})$, $0\leq i\leq n-1$;
    \item $S_{i+1}^\prime(x_{i+1}) = S_i^\prime(x_{i+1})$ for $0\leq i\leq n-1$;
    \item $S_{i+1}^{\prime\prime}(x_{i+1}) = S_i^{\prime\prime}(x_{i+1})$ for $0\leq i\leq n-1$;
    \item $S^{\prime\prime}(x_0) = S^{\prime\prime}(x_n) = 0$.
\end{enumerate}
Denote $c_i = S^{\prime\prime}(x_i),\,0\leq i\leq n$. Then $c_n=c_0=0$. Then
\begin{equation}
    S^{\prime\prime}_i = c_i\frac{x_{i+1} - x}{x_{i+1} - x_i} + c_{i+1}\frac{x-x_i}{x_{i+1} - x_i}.
\end{equation}
Integrate twice using boundary conditions to fix the integration constants.


\section{Different schemes for the heat equation, consistency}
\subsection{Lecture 25}
\subsubsection{Consistency}
A scheme is said to be consistent if it solves the PDE problem.

\subsubsection{Stability}
A numerical algorithm is said to be stable provided small errors in arithmetic remain bounded independently of the number of numerical steps used to reach the same value estimates.

\subsubsection{Convergence}
A scheme converges if it is both consistent and stable as the temporal and spatial time-steps are reduced.

\subsubsection{Four Finite Difference Schemes for the Heat Equation}
The performance of the four schemes is summarised as follows:
\begin{enumerate}
    \item The Euler explicit scheme converges if $\delta t$ is sufficiently small with respect to $(\delta x)^2$ but otherwise will blow-up.
    \item The Richardson scheme seems to be unstable for all choices of the Courant number $r$, so it will not converge.
    \item The Dufort-Frankel scheme converges but not necessarily to the correct solution as it is not consistent with the heat equation.
    \item The Crank-Nicolson scheme seems to exhibit stable behaviour for all choices of the Courant number and converges to the correct solution as spatial resolution is improved.
\end{enumerate}


\section{Consistency}
\subsection{Lecture 26}
Refer to lecture notes.


\section{Convergence}
\subsection{Lecture 27}
\subsubsection{Local Error}
denote $u$ the exact solution and $v$ the solution of the explicit Euler scheme to the equation
\begin{equation}
    u_t = u_{xx},\quad u(0,t) = u(1,t) = 0,\quad u(x,0) = u_0(x).
\end{equation}
\begin{proposition}
    Let
    \begin{equation}
        e_{i,j} = v_{i,j} - u(x_i,\,t_j)\nonumber
    \end{equation}
    be the local error at each grid point for the current set-up. Recall that $(\delta t) = r(\delta x)^2$, where $r$ is a constant.\\
    Then, the local error satisfies the following approximate iterative process:
    \begin{align}
        e_{i,j+1} &= (1-2r)e_{i,j} + re_{i+1,j} + re_{i-1,j} \nonumber\\
        &+ \underbrace{\frac{(\delta t)(\delta x)^2}{12}(1-6r)(u_{tt})_{i,j}}_{\text{leading approximation term}} + O((\delta t)^3,(\delta t)(\delta x)^4). \nonumber
    \end{align}
\end{proposition}
We can see that choosing $r=1/6$ improves the accuracy by removing the leading error term. Proof in lecture notes.

\subsection{Lecture 28}
\subsubsection{Error Along Time Layers for the Explicit Euler Scheme for the Heat Equation}
Define $E_j$ to be the spatial discretion error at a time $t_j$, that is,
\begin{equation}
    E_j = \max\{ \vert e_{i,j}\vert : 0\leq i \leq M \} = \Vert e_{i,j}\Vert_\infty \nonumber
\end{equation}
and let $\Vert u_{tt}\Vert_\infty$ be the maximum value of $u_{tt}$ across space and time on the rectangle $[0,\,1]\times[0,\,T]$:
\begin{equation}
    \Vert u_{tt}\Vert = \{ \vert u_{tt}(x,t):(x,t)\in[a,b]\times[0,T] \}.
\end{equation}    
Our main result is as follows:
\begin{proposition}
    With the previous notation, when $r\leq 1/2$, the error of the time-layer at time $T$ is
    \begin{equation}
        E_N\leq T\left( \frac{(\delta x)^2 \vert 1-6r\vert\cdot\Vert u_{tt}\Vert_\infty }{12} + O((\delta t)^2,(\delta x)^4) \right). \nonumber
    \end{equation}
\end{proposition}

\subsubsection{Consequences for Convergence}
\begin{corollary}
    Provided that $r\leq 1/2$:
    \begin{enumerate}
        \item The maximum spatial discretisation error in the Euler scheme grows in direct proportion to $T$, but
        \item For a fixed $T$, the error tends to $0$ as $\delta x$ tends to $0$.
        \item Remark that the explicit Euler scheme is more accurate than might otherwise have been anticipated when $r=1/6$ because the first part of the error vanishes.
    \end{enumerate}
\end{corollary}


\section{Stability Fourier method}
\subsection{Lecture 29}
Use lecture notes.
\subsection{Lecture 30}
Use lecture notes.
\subsection{Lecture 31}
Use lecture notes.


\section{Stability matrix method}
\subsection{Lecture 32}
\subsubsection{Eigenvalues of Banded Matrices}
A matrix is said to be banded (or Toeplitz) if the values along each diagonal are constant. 

\subsubsection{Circulant Matrices}
An $m\times m$-banded matrix is circulant if $a_i = a_{i-m},\,\forall 1\leq i \leq m$. The rows of a circulant $n \times n$-matrix are obtained by shifting a sequence of $n$ numbers by $m$ places.

\begin{proposition}
    The tri-diagonal circulant matrix $(m+1)\times(m+1)$-matrix
    \begin{equation}
        A =
        \begin{pmatrix}
            a_0 & a_1 & & & a_{-1} \\
            a_{-1} & a_0 & a_1 & & \\
            & a_{-1} & a_0 & a_1 & \\
            & & \vdots & \vdots & a_1 \\
            a_1 & & & a_{-1} & a_0 
        \end{pmatrix} \nonumber
    \end{equation}
    has eigenvalues
    \begin{equation}
        \lambda_p = a_{-1}e^{-\frac{2\pi p}{m+1}i} + a_0 + a_1 e^{\frac{2\pi p}{m+1}i},\quad 1\leq p\leq m+1,
    \end{equation}
    corresponding to the eigenvector $\underline{v}_p$ having its $j$-th component equal to $e^{\frac{2\pi jp}{m+1}i},\,1\leq j\leq m+1$. 
\end{proposition}
\begin{corollary}
    The tri-diagonal circulant $(m+1)\times(m+1)$-matrix 
    \begin{equation}
        A =
        \begin{pmatrix}
            a & b & & & b \\
            b & a & b & & \\
            & b & a & b & \\
            & & \ddots & \ddots & b \\
            b & & & b & a 
        \end{pmatrix}, \nonumber
    \end{equation}
    has eigenvalues
    \begin{equation}
        \lambda_p = a+2b\cos{\left( \frac{2\pi p}{m+1} \right)},\quad 1\leq p\leq m+1.
    \end{equation}
\end{corollary}

\subsubsection{Tri-diagonal Matrices}
\begin{proposition}
    Let $bc>0$. The tri-diagonal $(m+1)\times(m+1)$-matrix
    \begin{equation}
        \begin{pmatrix}
            a & b & 0 & 0 & \cdots & 0 \\
            c & a & b & 0 & \cdots & 0 \\
            0 & c & a & b & \cdots & 0 \\
            \vdots & \vdots & \ddots & \ddots & \ddots & 0 \\
            0 & \cdots & 0 & c & a & b \\
            0 & \cdots & 0 & 0 & c & a 
        \end{pmatrix} \nonumber
    \end{equation}
    has eigenvalues
    \begin{equation}
        \lambda_k = a + 2\sqrt{bc}\cos{\theta_k},\quad k = 1,\ldots,\,n, \nonumber
    \end{equation}
    where $\lambda_k = k\pi/(n+1)$, with corresponding eigenvector
    \begin{equation}
        \underline{v}_k = ( \sqrt{c/b}\sin{\theta_k},\,(c/b)\sin{(2\theta_k)},\ldots,\,(c/b)^{j/2}\sin{(j\theta_k)},\ldots,\,(c/b)^{n/2}\sin{(n\theta_k)} ). \nonumber
    \end{equation}
\end{proposition}
\begin{corollary}
    The tri-diagonal symmetric $(m+1)\times(m+1)$-matrix 
    \begin{equation}
        A =
        \begin{pmatrix}
            a & b & & &  \\
            b & a & b & & \\
            & b & a & b & \\
            & & \ddots & \ddots & b \\
            & & & b & a 
        \end{pmatrix}, \nonumber
    \end{equation}
    has eigenvalues
    \begin{equation}
        \lambda_p = a+2b\cos{\left( \frac{p\pi}{m+1} \right)},\quad 1\leq p\leq m,
    \end{equation}
    corresponding to the eigenvector having its $j$-th component equal to $\sin{\left( \frac{jp\pi}{m+1} \right)},\, 1\leq j\leq m+1$.
\end{corollary}

\subsection{Lecture 33}
\subsubsection{Statement of Gershgorin's Circle Theorem}
\begin{theorem}
    Let $A$ be an $n\times n$-matrix and the disks
    \begin{equation}
        D_k = \{ z\in \mathbb{C} : \vert z-A_{kk}\vert \leq \sum_{j=1\neq k} \vert A_{kj}\vert \}. \nonumber
    \end{equation}
    Then, all eigenvalues of $A$ are contained within the region $D = \bigcup_{k=1}^n D_k$. The disk $D_k$ is called a Gershgorin's disk. Its boundary is a Gershgorin's circle, denoted by $C_k$.\\
    In particular, if $m\leq n$ of these disks form a subset of $D$ which does not intersect the remaining $(n-m)$ disks, then precisely $m$ eigenvalues are contained within such a subset.
\end{theorem}

\subsubsection{Gershgorin's Circle Theorem and Transpose Matrix}
\begin{lemma}
    Given an $n\times n$-real matrix $A$, then $\sigma (A) = \sigma (A^T)$. \nonumber 
\end{lemma}

\subsection{Lecture 34}
\subsubsection{Diagonally Dominant Matrices}
\begin{definition}
    A matrix such that $\vert A_{kk}\vert > \sum_{j=1\neq k}^n \vert A_{kj} \vert$ for all $1\leq k \leq n$, is said to be strictly diagonally dominant.
\end{definition}
\begin{proposition}
    Strictly dominant diagonal matrices are invertible.
\end{proposition}

\subsubsection{Lax Theorem}
A boundary value problem is properly posed if:
\begin{enumerate}
    \item The solution is unique when it exists (it has at most one solution);
    \item The solution depends continuously on the initial data;
    \item When a solution does not exist for some initial data, there exists arbitrarily close initial data for which a solution does exist.
\end{enumerate}
\begin{theorem}
    Given a properly posed linear boundary value problem and a linear finite difference approximation scheme that is consistent with the BVP and stable, then the scheme is convergent.
\end{theorem}



\end{document}
